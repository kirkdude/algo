#!/usr/bin/env python3
"""
Quantum-Safe Cryptography Performance Monitor
Generated by Algo VPN quantum-safe role
"""

import json
import subprocess
import time
import psutil
import logging
from datetime import datetime
from pathlib import Path

class QuantumSafePerformanceMonitor:
    def __init__(self):
        self.config = {
            'strongswan_bin': '{{ strongswan_install_dir }}/sbin',
            'liboqs_lib': '/usr/local/lib/liboqs.so',
            'metrics_dir': '/opt/quantum-safe/metrics',
            'log_file': '/opt/quantum-safe/logs/performance.log',
            'monitoring_interval': 60  # seconds
        }

        # Setup logging
        logging.basicConfig(
            filename=self.config['log_file'],
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )

        self.logger = logging.getLogger(__name__)

        # Create metrics directory
        Path(self.config['metrics_dir']).mkdir(parents=True, exist_ok=True)

    def run_command(self, cmd, timeout=30):
        """Execute command and return result"""
        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True,
                timeout=timeout, shell=isinstance(cmd, str)
            )
            return {
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'returncode': result.returncode
            }
        except subprocess.TimeoutExpired:
            return {'success': False, 'error': 'Command timeout'}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def collect_system_metrics(self):
        """Collect system performance metrics"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory': dict(psutil.virtual_memory()._asdict()),
            'disk_usage': dict(psutil.disk_usage('/opt/quantum-safe')._asdict()),
            'load_average': psutil.getloadavg(),
            'boot_time': psutil.boot_time()
        }

    def collect_strongswan_metrics(self):
        """Collect strongSwan specific metrics"""
        metrics = {
            'service_status': 'unknown',
            'connections': [],
            'connection_count': 0,
            'algorithm_support': {}
        }

        # Check service status
        systemctl_result = self.run_command(['systemctl', 'is-active', 'strongswan'])
        metrics['service_status'] = 'active' if systemctl_result['success'] else 'inactive'

        ipsec_bin = f"{self.config['strongswan_bin']}/ipsec"

        # Check if binary exists
        if not Path(ipsec_bin).exists():
            metrics['binary_status'] = 'missing'
            return metrics

        metrics['binary_status'] = 'present'

        # Get strongSwan status
        status_result = self.run_command([ipsec_bin, 'statusall'])
        if status_result['success']:
            status_output = status_result['stdout']

            # Count connections
            established_connections = status_output.count('ESTABLISHED')
            metrics['connection_count'] = established_connections

            # Parse connection details
            for line in status_output.split('\n'):
                if 'ESTABLISHED' in line:
                    metrics['connections'].append({
                        'line': line.strip(),
                        'timestamp': datetime.now().isoformat()
                    })

        # Check algorithm support
        swanctl_bin = f"{self.config['strongswan_bin']}/swanctl"
        if Path(swanctl_bin).exists():
            alg_result = self.run_command([swanctl_bin, '--list-algs'])
            if alg_result['success']:
                alg_output = alg_result['stdout'].lower()

                # Check for quantum-safe algorithms
                pq_algorithms = [
                    {% for alg in quantum_safe_algorithms.ml_kem %}'{{ alg.lower() }}',{% endfor %}
                ]

                for alg in pq_algorithms:
                    metrics['algorithm_support'][alg] = alg in alg_output

        return metrics

    def collect_liboqs_metrics(self):
        """Collect LibOQS metrics"""
        metrics = {
            'library_status': 'unknown',
            'load_test': 'unknown',
            'file_info': {}
        }

        lib_path = Path(self.config['liboqs_lib'])

        if lib_path.exists():
            metrics['library_status'] = 'present'
            stat = lib_path.stat()
            metrics['file_info'] = {
                'size': stat.st_size,
                'modified': stat.st_mtime,
                'permissions': oct(stat.st_mode)
            }

            # Test library loading
            python_test = self.run_command([
                'python3', '-c',
                f'import ctypes; lib = ctypes.CDLL("{self.config["liboqs_lib"]}"); print("Load successful")'
            ])

            metrics['load_test'] = 'success' if python_test['success'] else 'failed'
        else:
            metrics['library_status'] = 'missing'

        return metrics

    def collect_network_metrics(self):
        """Collect network performance metrics"""
        metrics = {
            'interfaces': {},
            'connections': len(psutil.net_connections()),
            'io_counters': dict(psutil.net_io_counters()._asdict())
        }

        # Interface statistics
        for interface, stats in psutil.net_if_stats().items():
            metrics['interfaces'][interface] = {
                'isup': stats.isup,
                'duplex': stats.duplex.name if hasattr(stats.duplex, 'name') else str(stats.duplex),
                'speed': stats.speed,
                'mtu': stats.mtu
            }

        return metrics

    def benchmark_algorithm_performance(self):
        """Run basic performance benchmarks for quantum-safe algorithms"""
        benchmarks = {
            'timestamp': datetime.now().isoformat(),
            'results': {}
        }

        # This would ideally test actual algorithm performance
        # For now, we'll just record that the benchmark was attempted
        benchmarks['results']['note'] = 'Benchmark framework placeholder - requires liboqs test binaries'

        return benchmarks

    def generate_report(self):
        """Generate comprehensive performance report"""
        timestamp = datetime.now()

        report = {
            'timestamp': timestamp.isoformat(),
            'hostname': subprocess.getoutput('hostname'),
            'monitoring_version': '1.0',
            'system': self.collect_system_metrics(),
            'strongswan': self.collect_strongswan_metrics(),
            'liboqs': self.collect_liboqs_metrics(),
            'network': self.collect_network_metrics(),
            'benchmarks': self.benchmark_algorithm_performance()
        }

        # Save detailed JSON report
        report_file = Path(self.config['metrics_dir']) / f'performance-{timestamp.strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)

        # Log summary
        self.logger.info(f"Performance report generated: {report_file}")
        self.logger.info(f"System CPU: {report['system']['cpu_percent']:.1f}%, "
                        f"Memory: {report['system']['memory']['percent']:.1f}%, "
                        f"Connections: {report['strongswan']['connection_count']}")

        return report

    def continuous_monitoring(self):
        """Run continuous performance monitoring"""
        self.logger.info("Starting continuous quantum-safe performance monitoring")

        try:
            while True:
                report = self.generate_report()

                # Check for performance issues
                cpu_usage = report['system']['cpu_percent']
                memory_usage = report['system']['memory']['percent']

                if cpu_usage > 80:
                    self.logger.warning(f"High CPU usage detected: {cpu_usage:.1f}%")

                if memory_usage > 85:
                    self.logger.warning(f"High memory usage detected: {memory_usage:.1f}%")

                if report['strongswan']['service_status'] != 'active':
                    self.logger.error("strongSwan service is not active")

                if report['liboqs']['library_status'] != 'present':
                    self.logger.error("LibOQS library not found")

                # Sleep until next monitoring cycle
                time.sleep(self.config['monitoring_interval'])

        except KeyboardInterrupt:
            self.logger.info("Performance monitoring stopped by user")
        except Exception as e:
            self.logger.error(f"Performance monitoring error: {e}")

def main():
    monitor = QuantumSafePerformanceMonitor()

    if len(sys.argv) > 1:
        if sys.argv[1] == '--continuous':
            monitor.continuous_monitoring()
        elif sys.argv[1] == '--json':
            report = monitor.generate_report()
            print(json.dumps(report, indent=2, default=str))
        else:
            print("Usage: python3 performance-monitor.py [--continuous|--json]")
    else:
        # Single report
        report = monitor.generate_report()
        print(f"Performance report generated at: {datetime.now()}")
        print(f"CPU: {report['system']['cpu_percent']:.1f}%")
        print(f"Memory: {report['system']['memory']['percent']:.1f}%")
        print(f"strongSwan connections: {report['strongswan']['connection_count']}")
        print(f"LibOQS status: {report['liboqs']['library_status']}")

if __name__ == "__main__":
    import sys
    main()
